{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d83a97",
   "metadata": {},
   "source": [
    "# Multi-Model Bongard Problem Difficulty Analysis\n",
    "\n",
    "Analyzes all model results to find:\n",
    "- **Top 100 worst performing problems** (lowest success rate across models)\n",
    "- **Models that did well in top 100 worst performing problems**\n",
    "- Overall model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54c4e19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f62244d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - look in results directory\n",
    "results_dir = \"results/\"\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "def analyze_all_models(results_dir):\n",
    "    \"\"\"Analyze all JSON files in results directory.\"\"\"\n",
    "    \n",
    "    # Find all JSON files in results directory\n",
    "    json_files = glob.glob(os.path.join(results_dir, \"*.json\"))\n",
    "    print(f\"Found {len(json_files)} JSON files in {results_dir}\")\n",
    "    \n",
    "    if not json_files:\n",
    "        print(\"No JSON files found!\")\n",
    "        return {}, {}\n",
    "    \n",
    "    # Track results per problem and per model\n",
    "    problem_results = defaultdict(lambda: {'correct': 0, 'total': 0, 'models_correct': [], 'models_wrong': []})\n",
    "    model_accuracy = {}\n",
    "    \n",
    "    for file_path in json_files:\n",
    "        model_name = os.path.basename(file_path).replace('.json', '')\n",
    "        \n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # Check if data is a list\n",
    "            if not isinstance(data, list):\n",
    "                continue\n",
    "                \n",
    "            correct = 0\n",
    "            processed = 0\n",
    "            \n",
    "            for entry in data:\n",
    "                # Check if entry is a dict and has required fields\n",
    "                if not isinstance(entry, dict):\n",
    "                    continue\n",
    "                if 'uid' not in entry or 'answer' not in entry:\n",
    "                    continue\n",
    "                    \n",
    "                uid = entry['uid']\n",
    "                answer = entry['answer']\n",
    "                \n",
    "                # Determine expected answer based on UID suffix\n",
    "                if uid.endswith('A'):\n",
    "                    expected = 'positive'\n",
    "                elif uid.endswith('B'):\n",
    "                    expected = 'negative'\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                processed += 1\n",
    "                \n",
    "                # Track per problem\n",
    "                problem_results[uid]['total'] += 1\n",
    "                if answer == expected:\n",
    "                    correct += 1\n",
    "                    problem_results[uid]['correct'] += 1\n",
    "                    problem_results[uid]['models_correct'].append(model_name)\n",
    "                else:\n",
    "                    problem_results[uid]['models_wrong'].append(model_name)\n",
    "            \n",
    "            if processed > 0:\n",
    "                # Track per model\n",
    "                accuracy = (correct / processed) * 100\n",
    "                model_accuracy[model_name] = {\n",
    "                    'correct': correct,\n",
    "                    'total': processed,\n",
    "                    'accuracy': accuracy\n",
    "                }\n",
    "                \n",
    "        except Exception as e:\n",
    "            pass  # Silently skip errors\n",
    "    \n",
    "    return problem_results, model_accuracy\n",
    "\n",
    "def find_worst_problems(problem_results, top_n=100):\n",
    "    \"\"\"Find the worst performing problems.\"\"\"\n",
    "    \n",
    "    if not problem_results:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    problem_stats = []\n",
    "    for uid, stats in problem_results.items():\n",
    "        success_rate = (stats['correct'] / stats['total']) * 100 if stats['total'] > 0 else 0\n",
    "        problem_stats.append({\n",
    "            'UID': uid,\n",
    "            'Success_Rate': success_rate,\n",
    "            'Correct': stats['correct'],\n",
    "            'Total': stats['total'],\n",
    "            'Models_Correct': ', '.join(stats['models_correct']) if stats['models_correct'] else 'None'\n",
    "        })\n",
    "    \n",
    "    # Sort by success rate (worst first)\n",
    "    problem_stats.sort(key=lambda x: x['Success_Rate'])\n",
    "    \n",
    "    return pd.DataFrame(problem_stats[:top_n])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8bb8c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 JSON files in results/\n",
      "No JSON files found!\n",
      "No valid model results found!\n"
     ]
    }
   ],
   "source": [
    "# Run the analysis\n",
    "problem_results, model_accuracy = analyze_all_models(results_dir)\n",
    "\n",
    "if model_accuracy:\n",
    "    print(f\"MODEL ACCURACY SUMMARY:\")\n",
    "    print(\"-\" * 50)\n",
    "    model_df = pd.DataFrame.from_dict(model_accuracy, orient='index').sort_values('accuracy')\n",
    "    display(model_df)\n",
    "\n",
    "    # Find worst performing problems\n",
    "    worst_problems = find_worst_problems(problem_results, top_n=100)\n",
    "\n",
    "    if not worst_problems.empty:\n",
    "        # Add SCMR column\n",
    "        worst_problems['SDR_GPT41_Correct'] = worst_problems['Models_Correct'].apply(\n",
    "            lambda x: 'Yes' if 'sdr_gpt41_gpt41' in str(x) else 'No'\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nTOP 100 WORST PERFORMING PROBLEMS:\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Show all rows without truncation\n",
    "        # with pd.option_context('display.max_rows', None, 'display.max_colwidth', None):\n",
    "        #     display(worst_problems)\n",
    "        \n",
    "    else:\n",
    "        print(\"No problems to analyze!\")\n",
    "else:\n",
    "    print(\"No valid model results found!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d8891fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'worst_problems' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Which models perform best on the worst 100 problems\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mworst_problems\u001b[49m.empty \u001b[38;5;129;01mand\u001b[39;00m problem_results:\n\u001b[32m      4\u001b[39m     worst_100_uids = \u001b[38;5;28mset\u001b[39m(worst_problems[\u001b[33m'\u001b[39m\u001b[33mUID\u001b[39m\u001b[33m'\u001b[39m].tolist())\n\u001b[32m      6\u001b[39m     \u001b[38;5;66;03m# Track each model's performance on worst problems\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'worst_problems' is not defined"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Which models perform best on the worst 100 problems\n",
    "if not worst_problems.empty and problem_results:\n",
    "    worst_100_uids = set(worst_problems['UID'].tolist())\n",
    "    \n",
    "    # Track each model's performance on worst problems\n",
    "    model_performance_on_worst = defaultdict(lambda: {'correct': 0, 'total': 0})\n",
    "    \n",
    "    for uid in worst_100_uids:\n",
    "        if uid in problem_results:\n",
    "            stats = problem_results[uid]\n",
    "            \n",
    "            for model in stats['models_correct']:\n",
    "                model_performance_on_worst[model]['correct'] += 1\n",
    "                model_performance_on_worst[model]['total'] += 1\n",
    "            \n",
    "            for model in stats['models_wrong']:\n",
    "                model_performance_on_worst[model]['total'] += 1\n",
    "    \n",
    "    # Create results\n",
    "    results = []\n",
    "    for model, stats in model_performance_on_worst.items():\n",
    "        if stats['total'] > 0:\n",
    "            success_rate = (stats['correct'] / stats['total']) * 100\n",
    "            results.append({\n",
    "                'Model': model,\n",
    "                'Correct': stats['correct'],\n",
    "                'Total': stats['total'],\n",
    "                'Success_Rate': round(success_rate, 2)\n",
    "            })\n",
    "    \n",
    "    # Sort by success rate and display\n",
    "    results.sort(key=lambda x: x['Success_Rate'], reverse=True)\n",
    "    performance_df = pd.DataFrame(results)\n",
    "    \n",
    "    print(\"MODEL PERFORMANCE ON WORST 100 PROBLEMS:\")\n",
    "    display(performance_df)\n",
    "else:\n",
    "    print(\"No data available for analysis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5f63ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Find problems that sdr_gpt41_gpt41 got wrong\n",
    "\n",
    "if problem_results:\n",
    "    sdr_wrong_problems = []\n",
    "    ll = []\n",
    "    for uid, stats in problem_results.items():\n",
    "        if 'sdr_gpt41_gpt41' in stats['models_wrong']:\n",
    "            cvr_correct = 'Yes' if 'cvr_gpt41' in stats['models_correct'] else 'No'\n",
    "            scmr2_correct = 'Yes' if 'scmr2_gpt41_gpt41' in stats['models_correct'] else 'No'\n",
    "            if cvr_correct == 'Yes' and scmr2_correct == 'Yes':\n",
    "                ll.append(uid)\n",
    "            sdr_wrong_problems.append({\n",
    "                'UID': uid,\n",
    "                'CVR_GPT41_Correct': cvr_correct,\n",
    "                'SCMR2_GPT41_Correct': scmr2_correct,\n",
    "\n",
    "            })\n",
    "    print(ll.sort())\n",
    "    print(len(ll))\n",
    "    \n",
    "    if sdr_wrong_problems:\n",
    "        sdr_wrong_df = pd.DataFrame(sdr_wrong_problems).sort_values('UID').reset_index(drop=True)\n",
    "        \n",
    "        print(f\"PROBLEMS THAT SDR_GPT41_GPT41 GOT WRONG ({len(sdr_wrong_problems)} total):\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Define styling function\n",
    "        def highlight_both_correct(row):\n",
    "            if row['CVR_GPT41_Correct'] == 'Yes' and row['SCMR2_GPT41_Correct'] == 'Yes':\n",
    "                return ['background-color: purple'] * len(row)\n",
    "            else:\n",
    "                return [''] * len(row)\n",
    "        \n",
    "        with pd.option_context('display.max_rows', None, 'display.max_colwidth', None):\n",
    "            display(sdr_wrong_df.style.apply(highlight_both_correct, axis=1))\n",
    "        \n",
    "    else:\n",
    "        print(\"SDR_GPT41_GPT41 got all problems correct!\")\n",
    "else:\n",
    "    print(\"No valid problem results found!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
